// LLM å®¢æˆ·ç«¯ä¸­é—´ä»¶ - é™æµ/é‡è¯•/ç†”æ–­/é™çº§
import { EventEmitter } from 'events';

// === é…ç½® ===
export const LLM_CONFIG = {
  MAX_CONCURRENCY_PER_MODEL: 3, // æ¯æ¨¡å‹æœ€å¤§å¹¶å‘
  MAX_QUEUE_LENGTH: 15, // æœ€å¤§æ’é˜Ÿé•¿åº¦
  RETRY_MAX: 2, // æœ€å¤§é‡è¯•æ¬¡æ•° (å‡å°‘ä»¥åŠ å¿«å¤±è´¥åé¦ˆ)
  RETRY_BASE_MS: 500, // é‡è¯•åŸºç¡€å»¶è¿Ÿ (å‡å°‘ç­‰å¾…)
  RETRY_MAX_MS: 10000, // æœ€å¤§é‡è¯•å»¶è¿Ÿ
  TIMEOUT_CONNECT_MS: 15000, // è¿æ¥è¶…æ—¶ (å¢åŠ )
  TIMEOUT_READ_MS: 180000, // è¯»å–è¶…æ—¶ (3åˆ†é’Ÿï¼Œthinking æ¨¡å‹éœ€è¦æ›´é•¿)
  CIRCUIT_BREAKER_THRESHOLD: 3, // ç†”æ–­é˜ˆå€¼ (é™ä½ä»¥æ›´å¿«åˆ‡æ¢)
  CIRCUIT_BREAKER_RESET_MS: 30000, // ç†”æ–­æ¢å¤æ—¶é—´ (30ç§’åé‡è¯•)
  FALLBACK_MODELS: { // é™çº§é“¾
    'claude-opus-4-5-thinking': ['claude-sonnet-4-5-thinking', 'claude-sonnet-4-5', 'deepseek-chat', 'glm-4.7-flashx'],
    'claude-sonnet-4-5-thinking': ['claude-sonnet-4-5', 'deepseek-chat', 'glm-4.7-flashx'],
    'gemini-3-pro-high': ['gemini-3-flash', 'gemini-2.5-flash', 'deepseek-chat'],
    'deepseek-reasoner': ['deepseek-chat', 'glm-4.7-flashx'],
    'glm-4.7': ['glm-4.7-flashx', 'deepseek-chat'],
    // ç‰¹ä»·æ¸ é“é™çº§é“¾ï¼ˆå†…éƒ¨é™çº§ï¼Œä¸è·¨æ¸ é“ï¼‰
    'codesuc-opus': ['codesuc-sonnet', 'codesuc-haiku'],
    'codesuc-sonnet': ['codesuc-haiku'],
    'special-claude-opus-4-5': ['codesuc-sonnet', 'codesuc-haiku'], // å…¼å®¹æ—§ ID
    'special-claude-sonnet-4-5': ['codesuc-haiku'],
  } as Record<string, string[]>,
};

// === é”™è¯¯åˆ†ç±» ===
export type LLMErrorType = 'rate_limit' | 'capacity' | 'timeout' | 'network' | 'auth' | 'unknown';
export interface LLMError { type: LLMErrorType; message: string; retryable: boolean; statusCode?: number; suggestFallback?: boolean; }

export function classifyError(error: any): LLMError {
  const msg = String(error?.message || error || '').toLowerCase();
  const code = error?.status || error?.statusCode || (msg.match(/(\d{3})/) || [])[1];
  if (code === 429 || msg.includes('rate') || msg.includes('limit') || msg.includes('too many')) return { type: 'rate_limit', message: 'è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨é‡è¯•', retryable: true, statusCode: 429 };
  if (code === 503 || msg.includes('capacity') || msg.includes('unavailable') || msg.includes('overloaded')) return { type: 'capacity', message: 'æ¨¡å‹å®¹é‡ä¸è¶³ï¼Œå»ºè®®åˆ‡æ¢ç¨³å®šæ¨¡å‹', retryable: true, statusCode: 503, suggestFallback: true };
  if (msg.includes('timeout') || msg.includes('timed out') || msg.includes('etimedout')) return { type: 'timeout', message: 'è¯·æ±‚è¶…æ—¶ï¼Œå¯èƒ½ä¸Šæ¸¸æ’é˜Ÿè¾ƒé•¿', retryable: true };
  if (msg.includes('econnrefused') || msg.includes('enotfound') || msg.includes('network')) return { type: 'network', message: 'ç½‘ç»œè¿æ¥å¤±è´¥', retryable: true };
  if (code === 401 || code === 403 || msg.includes('auth') || msg.includes('key')) return { type: 'auth', message: 'API å¯†é’¥æ— æ•ˆ', retryable: false };
  return { type: 'unknown', message: error?.message || 'æœªçŸ¥é”™è¯¯', retryable: false };
}

// === ç†”æ–­å™¨ ===
class CircuitBreaker {
  private failures = new Map<string, { count: number; openUntil: number }>();
  isOpen(model: string): boolean {
    const state = this.failures.get(model);
    if (!state) return false;
    if (Date.now() > state.openUntil) { this.failures.delete(model); return false; } // ç†”æ–­æ¢å¤
    return state.count >= LLM_CONFIG.CIRCUIT_BREAKER_THRESHOLD;
  }
  recordFailure(model: string): void {
    const state = this.failures.get(model) || { count: 0, openUntil: 0 };
    state.count++;
    if (state.count >= LLM_CONFIG.CIRCUIT_BREAKER_THRESHOLD) state.openUntil = Date.now() + LLM_CONFIG.CIRCUIT_BREAKER_RESET_MS;
    this.failures.set(model, state);
  }
  recordSuccess(model: string): void { this.failures.delete(model); }
  getState(model: string): { open: boolean; failCount: number } {
    const state = this.failures.get(model);
    return { open: this.isOpen(model), failCount: state?.count || 0 };
  }
}

// === è¯·æ±‚é˜Ÿåˆ— ===
interface QueuedRequest<T> { execute: () => Promise<T>; resolve: (v: T) => void; reject: (e: any) => void; model: string; }
class RequestQueue {
  private queues = new Map<string, QueuedRequest<any>[]>();
  private running = new Map<string, number>();
  async enqueue<T>(model: string, execute: () => Promise<T>): Promise<T> {
    const currentRunning = this.running.get(model) || 0;
    if (currentRunning < LLM_CONFIG.MAX_CONCURRENCY_PER_MODEL) return this.run(model, execute);
    const queue = this.queues.get(model) || [];
    if (queue.length >= LLM_CONFIG.MAX_QUEUE_LENGTH) throw new Error('è¯·æ±‚é˜Ÿåˆ—å·²æ»¡ï¼Œè¯·ç¨åé‡è¯•');
    return new Promise((resolve, reject) => { queue.push({ execute, resolve, reject, model }); this.queues.set(model, queue); });
  }
  private async run<T>(model: string, execute: () => Promise<T>): Promise<T> {
    this.running.set(model, (this.running.get(model) || 0) + 1);
    try { return await execute(); }
    finally { this.running.set(model, (this.running.get(model) || 1) - 1); this.processQueue(model); }
  }
  private processQueue(model: string): void {
    const queue = this.queues.get(model);
    if (!queue || queue.length === 0) return;
    const currentRunning = this.running.get(model) || 0;
    if (currentRunning >= LLM_CONFIG.MAX_CONCURRENCY_PER_MODEL) return;
    const next = queue.shift()!;
    this.run(model, next.execute).then(next.resolve).catch(next.reject);
  }
  getStats(): Record<string, { running: number; queued: number }> {
    const stats: Record<string, { running: number; queued: number }> = {};
    for (const [model, count] of this.running) stats[model] = { running: count, queued: this.queues.get(model)?.length || 0 };
    return stats;
  }
}

// === é‡è¯•é€»è¾‘ ===
async function withRetry<T>(fn: () => Promise<T>, model: string, breaker: CircuitBreaker): Promise<T> {
  let lastError: any;
  for (let attempt = 0; attempt <= LLM_CONFIG.RETRY_MAX; attempt++) {
    try {
      const result = await fn();
      breaker.recordSuccess(model);
      return result;
    } catch (error) {
      lastError = error;
      const classified = classifyError(error);
      if (!classified.retryable || attempt === LLM_CONFIG.RETRY_MAX) { breaker.recordFailure(model); throw error; }
      const delay = Math.min(LLM_CONFIG.RETRY_BASE_MS * Math.pow(2, attempt) + Math.random() * 500, LLM_CONFIG.RETRY_MAX_MS); // æŒ‡æ•°é€€é¿ + jitter
      await new Promise(r => setTimeout(r, delay));
    }
  }
  throw lastError;
}

// === é™çº§é€»è¾‘ ===
export function getFallbackModel(model: string, triedModels: Set<string>): string | null {
  const fallbacks = LLM_CONFIG.FALLBACK_MODELS[model] || [];
  for (const fb of fallbacks) if (!triedModels.has(fb)) return fb;
  return null;
}

// === LLM å®¢æˆ·ç«¯ ===
export interface LLMRequest { model: string; messages: any[]; stream?: boolean; tools?: any[]; userId?: string; requestId?: string; }
export interface LLMResponse { success: boolean; data?: string; model: string; usedFallback?: boolean; error?: LLMError; retryCount?: number; }
export interface LLMStreamCallbacks { onToken: (token: string) => void; onToolCall?: (calls: any[]) => void; onComplete: (text: string, meta: { model: string; usedFallback: boolean }) => void; onError: (error: LLMError) => void; onFallback?: (from: string, to: string) => void; }

export class LLMClient extends EventEmitter {
  private breaker = new CircuitBreaker();
  private queue = new RequestQueue();
  private providers: Map<string, any>;
  constructor(providers: Map<string, any>) { super(); this.providers = providers; }

  private getProviderForModel(model: string): any {
    let providerName = 'claude'; // é»˜è®¤
    if (model.startsWith('codesuc-') || model.startsWith('special-claude-')) providerName = 'codesuc'; // ç‰¹ä»·æ¸ é“ï¼ˆå…¼å®¹æ—§ IDï¼‰
    else if (model.startsWith('claude-')) providerName = 'claude';
    else if (model.startsWith('gemini-')) providerName = 'gemini';
    else if (model.startsWith('deepseek-')) providerName = 'deepseek';
    else if (model.startsWith('glm-')) providerName = 'glm';
    else if (model.startsWith('gpt-')) providerName = 'openai';
    console.log(`[LLM] getProviderForModel: model=${model} -> provider=${providerName}`);
    return this.providers.get(providerName);
  }

  async chat(request: LLMRequest): Promise<LLMResponse> {
    const triedModels = new Set<string>();
    let currentModel = request.model;
    while (true) {
      triedModels.add(currentModel);
      if (this.breaker.isOpen(currentModel)) { // ç†”æ–­ä¸­ï¼Œå°è¯•é™çº§
        const fallback = getFallbackModel(currentModel, triedModels);
        if (fallback) { this.emit('fallback', currentModel, fallback); currentModel = fallback; continue; }
        return { success: false, model: currentModel, error: { type: 'capacity', message: 'æ‰€æœ‰æ¨¡å‹æš‚ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•', retryable: false } };
      }
      try {
        const provider = this.getProviderForModel(currentModel);
        const data = await this.queue.enqueue<string>(currentModel, () => withRetry(() => provider.setModel(currentModel).chat(request.messages), currentModel, this.breaker));
        return { success: true, data, model: currentModel, usedFallback: currentModel !== request.model };
      } catch (error) {
        const classified = classifyError(error);
        if (classified.suggestFallback) {
          const fallback = getFallbackModel(currentModel, triedModels);
          if (fallback) { this.emit('fallback', currentModel, fallback); currentModel = fallback; continue; }
        }
        return { success: false, model: currentModel, error: classified };
      }
    }
  }

  async chatStream(request: LLMRequest, callbacks: LLMStreamCallbacks): Promise<void> {
    const triedModels = new Set<string>();
    let currentModel = request.model;
    const tryStream = async (): Promise<void> => {
      triedModels.add(currentModel);
      if (this.breaker.isOpen(currentModel)) {
        const fallback = getFallbackModel(currentModel, triedModels);
        if (fallback) { callbacks.onFallback?.(currentModel, fallback); currentModel = fallback; return tryStream(); }
        callbacks.onError({ type: 'capacity', message: 'æ‰€æœ‰æ¨¡å‹æš‚ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•', retryable: false }); return;
      }
      const provider = this.getProviderForModel(currentModel);
      return this.queue.enqueue(currentModel, () => new Promise<void>((resolve, reject) => {
        const streamFn = request.tools && provider.chatWithTools ? provider.chatWithTools.bind(provider) : provider.chatStream.bind(provider);
        const args = request.tools ? [request.messages, request.tools] : [request.messages];
        const wrappedCallbacks = {
          onToken: callbacks.onToken,
          onToolCall: callbacks.onToolCall,
          onComplete: (text: string) => { this.breaker.recordSuccess(currentModel); callbacks.onComplete(text, { model: currentModel, usedFallback: currentModel !== request.model }); resolve(); },
          onError: async (error: Error) => {
            const classified = classifyError(error);
            this.breaker.recordFailure(currentModel);
            if (classified.suggestFallback) {
              const fallback = getFallbackModel(currentModel, triedModels);
              if (fallback) { callbacks.onFallback?.(currentModel, fallback); currentModel = fallback; tryStream().then(resolve).catch(reject); return; }
            }
            callbacks.onError(classified); reject(error);
          }
        };
        provider.setModel(currentModel);
        streamFn(...args, wrappedCallbacks).catch((e: Error) => wrappedCallbacks.onError(e));
      }));
    };
    await tryStream();
  }

  getStats(): { queue: Record<string, { running: number; queued: number }>; breakers: Record<string, { open: boolean; failCount: number }> } {
    const breakers: Record<string, { open: boolean; failCount: number }> = {};
    for (const model of ['claude-opus-4-5-thinking', 'claude-sonnet-4-5', 'deepseek-chat', 'gemini-3-flash']) breakers[model] = this.breaker.getState(model);
    return { queue: this.queue.getStats(), breakers };
  }
}

// === ç”¨æˆ·å‹å¥½é”™è¯¯æ¶ˆæ¯ ===
export function getUserFriendlyError(error: LLMError): string {
  switch (error.type) {
    case 'rate_limit': return 'â³ è¯·æ±‚é™æµä¸­ï¼Œç³»ç»Ÿæ­£åœ¨é‡è¯•...ï¼ˆå¯åˆ‡æ¢ DeepSeek V3 è·å¾—æ›´å¿«å“åº”ï¼‰';
    case 'capacity': return 'âš ï¸ å½“å‰æ¨¡å‹ç¹å¿™ï¼å»ºè®®ï¼š1) åˆ‡æ¢åˆ° DeepSeek V3 2) ç¨åé‡è¯• 3) æ£€æŸ¥ API é…é¢';
    case 'timeout': return 'â±ï¸ è¯·æ±‚è¶…æ—¶ï¼ˆThinking æ¨¡å‹å“åº”è¾ƒæ…¢ï¼‰ã€‚å»ºè®®ï¼š1) ç­‰å¾… 30 ç§’åé‡è¯• 2) åˆ‡æ¢åˆ°æ›´å¿«çš„æ¨¡å‹';
    case 'network': return 'ğŸŒ ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ï¼š1) ç½‘ç»œè¿æ¥ 2) ä»£ç†è®¾ç½® 3) API åœ°å€';
    case 'auth': return 'ğŸ”‘ API å¯†é’¥æ— æ•ˆï¼Œè¯·åœ¨è®¾ç½®ä¸­æ£€æŸ¥ API Key é…ç½®';
    default: return error.message || 'âŒ è¯·æ±‚å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•';
  }
}
